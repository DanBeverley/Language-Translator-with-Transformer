{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U torchdata\n!pip install -U spacy\n!python -m spacy download en_core_web_sm\n!python -m spacy download de_core_news_sm\nimport torch\nfrom torch import nn\nfrom torch.nn import Transformer\nimport math\nfrom torch import Tensor\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\nfrom torchtext.datasets import multi30k , Multi30k\nfrom typing import Iterable , List\nfrom tqdm.notebook import tqdm\nfrom timeit import default_timer as timer\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nEPOCHS = 20\nBATCH_SIZE = 32\nEMBED_SIZE = 512\nNHEAD = 8\nFFN_HID_DIM = 512\nNUM_ENCODER_LAYERS = 3\nNUM_DECODER_LAYERS = 3\n\nmulti30k.URL[\"train\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\"\nmulti30k.URL[\"valid\"] = \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\"\n\nSRC_LANGUAGE = 'de'\nTGT_LANGUAGE = 'en'\n\n#Place holders\ntoken_transforms = {}\nvocab_transforms = {}\n\ntoken_transforms[SRC_LANGUAGE] = get_tokenizer('spacy',language = 'de_core_news_sm')\ntoken_transforms[TGT_LANGUAGE] = get_tokenizer('spacy',language = 'en_core_web_sm')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install 'portalocker'\n\n# helper function to yield list of tokens\ndef yield_tokens(data_iter: Iterable, language: str) -> List[str]:\n    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n\n    for data_sample in data_iter:\n        yield token_transform[language](data_sample[language_index[language]])\n\n# Define special symbols and indices\nUNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n# Make sure the tokens are in order of their indices to properly insert them in vocab\nspecial_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n\nfor ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n    # Training data Iterator\n    train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n    # Create torchtext's Vocab object\n    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter, ln),\n                                                    min_freq=1,\n                                                    specials=special_symbols,\n                                                    special_first=True)\n\n# Set ``UNK_IDX`` as the default index. This index is returned when the token is not found.\n# If not set, it throws ``RuntimeError`` when the queried token is not found in the Vocabulary.\nfor ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n  vocab_transform[ln].set_default_index(UNK_IDX)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Helper Module that adds positional encoding to the token embedding\nclass PositionalEncoding(nn.Module):\n    def __init__(self , emb_size : int ,\n                        dropout : float , \n                        maxlen:int = 5000):\n        super(PositionalEncoding , self).__init__()\n        den = torch.exp(-torch.arrange(0 , emb_size , 2)*math.log(10000)/emb_size)\n        pos = torch.arange(0,maxlen).reshape(maxlen , 1)\n        pos_embedding = torch.zeros((maxlen , emb_size))\n        pos_embedding[: , 0::2] = torch.sin(pos * den)\n        pos_embedding[: , 1::2] = torch.cos(pos * den)\n        pos_embedding = pos_embedding.unsqueeze(-2)\n        \n        self.dropout = nn.Dropout(dropout)\n        self.register_buffer('pos_embedding',pos_embedding)\n        \n    def forward(self , token_embedding:Tensor):\n        return self.dropout(token_embedding \n                            + self.pos_embedding[:token_embedding.size(0),\n                                                :])\n#Helper module to convert tensor of input indices into corresponding tensor of token embeddings\nclass TokenEmbedding(nn.Module):\n    def __init__(self , vocab_size:int , emb_size):\n        super(TokenEmbedding , self).__init__()\n        self.embedding = nn.Embedding(vocab_size , emb_size)\n        self.emb_size = emb_size\n    def forward(self , tokens:Tensor):\n        return self.embedding(tokens.long())*math.sqrt(self.emb_size)\n\n#Seq2Seq Network\nclass Seq2SeqTransformer(nn.Module):\n    def __init__(self , \n                num_encoder_layers:int,\n                num_decoder_layers:int,\n                emb_size:int,\n                nhead:int,\n                src_vocab_size:int,\n                tgt_vocab_size:int,\n                dim_feedforward:int = 512,\n                dropout:float = .1):\n        super(Seq2SeqTransformer,self).__init__()\n        self.transformer = Transformer(d_model = emb_size,\n                                      nhead = nhead,\n                                      num_encoder_layers = num_encoder_layers,\n                                      num_decoder_layers = num_decoder_layers,\n                                      dim_feedforward = dim_feedforward,\n                                      dropout = dropout)\n        self.generator = nn.Linear(emb_size , tgt_vocab_size)\n        self.src_tok_emb = TokenEmbedding(src_vocab_size , emb_size)\n        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size , emb_size)\n        self.positional_encoding = PositionalEncoding(emb_size , dropout = dropout)\n    def forward(self , src:Tensor , tgt:Tensor,\n                src_mask:Tensor , tgt_mask:Tensor,\n               src_padding_mask:Tensor , tgt_padding_mask:Tensor,\n               memory_key_padding_mask:Tensor):\n        src_emb = self.positional_encoding(self.src_tok_emb(src))\n        tgt_emb = self.positional_encoding(self.tgt_tok_emb(tgt))\n        outs = self.transformer(src_emb , tgt_emb , src_mask , tgt_mask , None,\n                               src_padding_mask , tgt_padding_mask ,\n                               memmory_key_padding_mask)\n        return self.generator(outs)\n    \n    def encode(self , src:Tensor , src_mask:Tensor,):\n        return self.transformer.encoder(self.positional_encoding(\n            self.src_tok_emb(src)),src_mask)\n    def decode(self , tgt:Tensor , memory:Tensor , tgt_mask:Tensor):\n        return self.transformer.decoder(self.positional_encoding(\n        self.tgt_tok_emb(tgt)) , memory , tgt_mask)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Subsequent work mask that prevent the model looking the future words\n\ndef generate_square_subsequent_mask(sz):\n    mask = (torch.triu(torch.ones((sz,sz) , device = device)) ==1).transpose(0,1)\n    mask = mask.float().masked_fill(mask == 0 , float('-inf').masked_fill(mask == 1,\n                                                                         float(0,0)))\n    return mask\n\ndef create_mask(src,tgt):\n    src_seq_len = src.shape[0]\n    tgt_seq_len = tgt.shape[0]\n    \n    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n    src_mask = torch.zeros((src_seq_len , src_seq_len) , device = device).type(torch.bool)\n    \n    src_padding_mask = (src == PAD_IDX).transpose(0,1)\n    tgt_padding_mask = (tgt == PAD_IDX).transpose(0,1)\n    \n    return src_mask , tgt_mask , src_padding_mask , tgt_padding_mask","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SRC_VOCAB_SIZE = len(vocab_transform[SRC_LANGUAGE])\nTGT_VOCAB_SIZE = len(vocab_transform[TGT_LANGUAGE])\n\ntransformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS , \n                                NUM_DECODER_LAYERS,\n                                EMB_SIZE,\n                                NHEAD,\n                                SRC_VOCAB_SIZE,\n                                TGT_VOCAB_SIZE,\n                                FFN_HID_DIM)\nfor p in transformer.parameters():\n    if p.dim()>1:\n        nn.init.xavier_uniform_(p)\ntransformer = transformer.to(device)\nloss_fn = torch.nn.CrossEntropyLoss(ignore_index = PAD_IDX)\noptimizer = torch.optim.Adam(transformer.parameters() , lr=3e-4 , \n                            betas = (.9 , .98) , eps = 1e-9)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Convert raw strings from data iterator to tensor for processing\n\nfrom torch.nn.utils.rnn import pad_sequence\n\n# Helper function to club together sequential operations\n\ndef sequential_transform(*transforms):\n    def func(txt_input):\n        for transform in transfomrs:\n            txt_input = transform(txt_input)\n        return txt_input\n    return func\n\n# Function to add BOS/EOS and create tensor for input sequence indices\n\ndef tensor_transform(token_ids:List[int]):\n    return torch.cat((torch.tensor([BOS_IDX]),\n                     torch.tensor(token_ids),\n                     torch.tensor([EOS_IDX])))\n\n# 'src' and 'tgt' language text transforms to convert raw swings to tensor indices\ntext_transform = {}\nfor ln in [SRC_LANGUAGE , TGT_LANGUAGE]:\n    text_transform[ln] = sequential_transform(token_transform[ln],#Tokenization\n                                             vocab_transform[ln], #Numericalization\n                                             tensor_transform)    #Add BOS/EOS and create tensor\n#Function to collate data samples into batch tensors\n\ndef collate_fn(batch):\n    src_batch , tgt_batch = [] , []\n    for src_batch , tgt_batch in batch:\n        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n    src_batch = pad_sequence(src_batch , padding_value = PAD_IDX)\n    tgt_batch = pad_sequence(tgt_batch , padding_value = PAD_IDX)\n    return src_batch , tgt_batch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ndef train_epoch(model , optimizer):\n    model.train()\n    losses = 0\n    train_iter = Multi30k(split = 'train' , language_pair = (SRC_LANGUAGE , TGT_LANGUAGE))\n    train_dataloader = DataLoader(train_iter , batch_size = BATCH_SIZE , collate_fn = collate_fn)\n    \n    for src , tgt in tqdm(len(train_dataloader)):\n        src , tgt = src.to(device) , tgt.to(device)\n        \n        tgt_input = tgt[:-1 , :]\n        \n        src_mask , tgt_mask , src_padding_mask ,tgt_padding_mask = create_mask(src , tgt_input)\n        \n        logits = model(src , tgt_input , src_mask , tgt_mask , src_padding_mask , \n                      tgt_padding_mask , src_padding_mask)\n        optimizer.zero_grad()\n        \n        tgt_out = tgt[1:,:]\n        loss = loss_fn(logits.reshape(-1 , logits.shape[-1]), tgt_out.reshape(-1))\n        loss.backward()\n        \n        optimizer.step()\n        losses += loss.item()\n    return losses/len(list(train_dataloader))\n\ndef evaluate(model):\n    model.eval()\n    losses = 0\n    \n    val_iter = Multi30k(split = 'valid' , language_pair = (SRC_LANGUAGE , TGT_LANGUAGE))\n    val_dataloader = DataLoader(val_iter , batch_size = BATCH_SIZE , collate_fn = collate_fn)\n    \n    for src , tgt in tqdm(len(val_dataloader)):\n        src , tgt = src.to(device) , tgt.to(device)\n        \n        tgt_input = tgt[:-1,:]\n        \n        src_mask , tgt_mask , src_padding_mask , tgt_padding_mask = create_mask(src,tgt_input)\n        logits = model(src , tgt_input , src_mask , tgt_mask , \n                      src_padding_mask , tgt_padding_mask,\n                      src_padding_mask)\n        tgt_out = tgt[1:,:]\n        loss = loss_fn(logits.reshape(-1,logits.shape[-1]),tgt_out.reshape(-1))\n        losses += loss.item()\n    return losses/len(list(val_dataloader))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(1, EPOCHS+1):\n    start_timer = timer()\n    train_loss = train_epoch(transformer , optimizer)\n    end_time = timer()\n    val_loss = evaluate(transformer)\n    \n    print((f\"Epoch : {epoch} |\n            Train Loss : {train_loss:.3f} |\n            Val Loss : {val_loss:.3f}\",\n         f\"Epoch Timer = {(end_time - start_timer):.3f}s\"))\n    \n#Generate output sequence using greedy algo\n\ndef greedy_decode(model , src , src_mask , max_len , start_symbol):\n    #In greedy decoding, at each step of generation, \n    #the model selects the token with the highest probability as \n    #the next token in the sequence, without considering future tokens.\n    \n    src = src.to(device)\n    src_mask = src_mask.to(device)\n    \n    memory = model.encode(src,src_mask)\n    ys = torch.ones(1,1).fill_(start_symbol).type(torch.long).to(device)\n    for i in range(max_len-1):\n        memory = memory.to(device)\n        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n                    .type(torch.bool)).to(device)\n        out = model.decode(ys,memory,tgt_mask)\n        out = out.transpose(0,1)\n        prob = model.generator(out[:,-1])\n        _ , next_word = torch.max(prob , dim = 1)\n        next_word = next_word.item()\n        \n        ys = torch.cat([ys,torch.ones(1,1).type_as(src.data).fill_(next_word)],\n                      dim = 0)\n        if next_word == EOS_IDX:\n            break\n    return ys","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def translate(model:torch.nn.Module , src_sentence:str):\n    model.eval()\n    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1,1)\n    num_tokens = src.shape[0]\n    src_mask = (torch.zeros(num_tokens,num_tokens)).type(torch.bool)\n    tgt_tokens = greedy_decode(\n    model , src , src_mask , max_len = num_tokens+5 , start_symbol = BOS_IDX).flatten()\n    return \" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\",\"\").replace(\"<eos>\",\"\") ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(translate(transformer , \"Ich liebe dich\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}